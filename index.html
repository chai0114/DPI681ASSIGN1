<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>DPI681M — Interactive Concepts (v2 Patched)</title>
  <link rel="preconnect" href="https://www.gutenberg.org">
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
<header>
  <div class="wrap navgrid">
    <h1>DPI681M — Interactive Concept Guide <span class="tag">v2 Patched</span></h1>
    <nav aria-label="Modules">
      <a href="#m1">Module 1: Basics</a>
      <a href="#m2">Module 2: Meaning</a>
      <a href="#m3">Module 3: Pipeline</a>
      <a href="#m4">Module 4: Application</a>
    </nav>
    <button class="btn" id="reset-all" title="Reset all interactions">Reset</button>
  </div>
</header>
<main class="wrap">
  <p class="small">Each scene includes: <strong>Concept & principle</strong>, a <strong>working interactive sketch</strong>, and <strong>Key Principles</strong> explaining what you're seeing. Goals & pitfalls are stated up front for efficient learning.</p>

  <h2 id="m1">Module 1: Basic Predictors</h2>

  <section id="s1" class="card">
    <div class="row">
      <div>
        <h3>1) Autocomplete — Next‑word Prediction</h3>
        <div class="status-box status-ok">Goals: estimate <span class="mono">P(next|context)</span> · read absolute probability bars · see how sampling/appending works.</div>
        <div class="status-box status-err">Pitfall: Do not confuse relative bar length with true probabilities (we show absolute % by default).</div>
        <p><strong>Concept.</strong> An LLM is a highly trained <em>next‑token predictor</em>. Given a context, it assigns probabilities to candidate next words.</p>
        <p><strong>Principle.</strong> Learn a distribution <span class="mono">P(next | context)</span> from text; generation is sampling from that distribution.</p>
        <div class="controls" aria-label="Controls for scene 1">
          <label>Prompt <input id="s1-prompt" class="kbd" size="36" value="Shall I compare thee to a"/></label>
          <button class="btn" id="s1-suggest">Suggest next</button>
          <button class="btn" id="s1-append">Append top word</button>
        </div>
        <details><summary>Key Principle (How it works)</summary>
          <p>We compute <span class="mono">P(next | context)</span> from the <em>full Shakespeare Sonnets</em> (Project Gutenberg, public domain).</p>
          <ol>
            <li>Tokenize to lowercase words (keep apostrophes).</li>
            <li>Build n‑gram counts up to 4‑grams (3‑token context).</li>
            <li><strong>Laplace (+1) smoothing</strong> is always on to avoid zeros.</li>
            <li><strong>Interpolated backoff</strong>: mix 3‑gram, 2‑gram, and 1‑gram distributions with weights λ₃=0.6, λ₂=0.3, λ₁=0.1 (renormalized if some levels are missing).</li>
            <li>Bars show absolute probabilities; <em>Append</em> inserts the top word and recomputes.</li>
          </ol>
          <p><strong>Tip:</strong> Start with <em>“Shall I compare thee to a”</em> and append; you should see <em>summer’s</em> dominate but not 100%, thanks to interpolation + smoothing.</p>
        </details>
      </div>
      <div>
        <figure class="sketch" id="s1-sketch" aria-live="polite"></figure>
        <figcaption class="small">Bars show absolute probabilities (0–100%). Computed from Shakespeare’s <em>Sonnets</em> using interpolated n‑gram backoff with Laplace smoothing.</figcaption>
        <div id="s1-meta" class="small mono" aria-live="polite" style="margin-top:.25rem"></div>
        <div id="s1-source" class="small" style="margin-top:.25rem"></div>
      </div>
    </div>
  </section>

  <section id="s3" class="card">
    <div class="row">
      <div>
        <h3>3–4) n‑grams — Short Memory Predictors</h3>
        <div class="status-box status-ok">Goals: experience exact‑match counting · compare n=2/3/4 · see smoothing reduce zeros.</div>
        <div class="status-box status-err">Pitfall: Larger n ≠ always better. Data sparsity explodes.</div>
        <p><strong>Concept.</strong> An n‑gram model only looks at the last <span class="mono">n−1</span> tokens. It counts how often contexts are followed by words.</p>
        <p><strong>Principle.</strong> Markov assumption with limited history; data sparsity grows with larger <span class="mono">n</span>.</p>
        <div class="controls">
          <label>n = <input id="s3-n" type="range" min="2" max="4" value="3"/> <span id="s3-nv" class="pill">3</span></label>
          <label>Context <input id="s3-context" class="kbd" size="28" value="i like"/></label>
          <label><input type="checkbox" id="s3-smooth"> Laplace smoothing (+1)</label>
          <button class="btn" id="s3-calc">Compute P(next | context)</button>
        </div>
        <details><summary>Key Principle (How it works)</summary>
          <p>We count n‑grams on a tiny corpus: <small>["i like pizza", "i like pasta", "you like pizza", "i like tea very much"]</small>.</p>
          <ol>
            <li>Check the context length = <span class="mono">n−1</span>.</li>
            <li>Find exact matches and count following words.</li>
            <li><strong>Smoothing on:</strong> add 1 to every vocabulary word (unseen words included); denominator adds |V|.</li>
          </ol>
          <p><strong>Try:</strong> n=2 context="like"; n=3 context="i like"; n=4 context="like tea very"; n=4 context="you like pizza" (sparsity → use smoothing).</p>
        </details>
      </div>
      <div>
        <figure class="sketch" id="s3-sketch"></figure>
        <div id="s3-table" class="small" style="margin-top:.5rem"></div>
        <div id="s3-corpus" class="small" style="margin-top:.25rem"></div>
        <figcaption class="small">Absolute probabilities; enable smoothing to avoid zeros.</figcaption>
      </div>
    </div>
  </section>

  <h2 id="m2">Module 2: Representing Meaning</h2>

  <section id="s5" class="card">
  <div class="row">
    <div>
      <h3>5–6) Word Embeddings — CBOW with Short Sentences</h3>
      <div class="status-box status-ok">Goals: see each word as a vector · average context vectors · predict the next word from cosine similarity.</div>
      <div class="status-box status-err">Pitfall: averaging is a teaching simplification; real models learn much richer context functions.</div>
      <p><strong>Concept.</strong> Each word is a vector. To predict the next word, take the <em>average</em> of the context vectors and choose the candidate whose vector is most similar (cosine).</p>
      <p><strong>Principle.</strong> This is the CBOW idea from word2vec: predict a target from surrounding words using the mean of their embeddings. (As in the class handout.)</p>
      <div class="controls">
        <label>Example 
          <select id="s5-example" class="kbd">
            <option value="sonnet">Shall I compare thee to a summer’s ___</option>
            <option value="directions">I should turn right at the light ___</option>
          </select>
        </label>
        <label><input id="s5-ignore-stops" type="checkbox" checked> Ignore stopwords</label>
        <button class="btn" id="s5-predict">Predict next word</button>
        <button class="btn" id="s5-reset-emb">Reset example</button>
      </div>
      <details><summary>Key Principle (How it works)</summary>
        <ol>
          <li>Map each token in the context to a 2-D vector (shown on the map).</li>
          <li>Compute the <strong>mean vector</strong> μ of the selected context tokens.</li>
          <li>For each candidate next word, compute cosine(μ, v<sub>candidate</sub>).</li>
          <li>Softmax over those cosines → bars below. Top item is appended.</li>
        </ol>
        <p class="small">This mirrors the “Average of other word vectors” CBOW diagram and the Shakespeare example from your slides. </p>
      </details>
    </div>
    <div>
      <canvas id="s5-map" width="520" height="340" aria-label="embedding map"></canvas>
      <figcaption class="small">Blue points = context words (click to toggle include). Orange star = mean μ. Hollow ring = top prediction. Bars = softmax over cosine scores.</figcaption>
      <div id="s5-bars" class="sketch" style="margin-top:.5rem"></div>
      <div id="s5-table" class="small mono" style="margin-top:.35rem" aria-live="polite"></div>
    </div>
  </div>
</section>


  <section id="s7" class="card">
  <div class="row">
    <div>
      <h3>7) Toy Neural Net — 2×2 Diagrammed Perceptron</h3>
      <div class="status-box status-ok">
        Goals: see a real 2×2 → 3 (hidden) → 4 (classes) network · watch weights, activations, probabilities.
      </div>
      <div class="status-box status-err">
        Pitfall: weights alone don’t explain everything—bias and ReLU matter.
      </div>
      <p><strong>Concept.</strong> A tiny 2-layer perceptron classifies 2×2 patterns.</p>
      <p><strong>Principle.</strong> Linear transforms + ReLU, then softmax over 4 classes.</p>
      <div class="controls">
        <button class="btn" id="s7-vertical">Vertical</button>
        <button class="btn" id="s7-horizontal">Horizontal</button>
        <button class="btn" id="s7-diagonal">Diagonal</button>
        <button class="btn" id="s7-none">None</button>
        <button class="btn" id="s7-all">All On (noise)</button>
        <label class="pill"><input type="checkbox" id="s7-show-w" checked> Show weights</label>
        <label class="pill"><input type="checkbox" id="s7-show-neg" checked> Show negatives</label>
      </div>
      <details><summary>Key Principle (How it works)</summary>
        <ol>
          <li>Inputs x ∈ {0,1}⁴ (2×2 pixel grid).</li>
          <li>Hidden pre-act z<sub>h</sub> = x·W<sub>ih</sub> + b<sub>h</sub> → h = ReLU(z<sub>h</sub>).</li>
          <li>Logits y = h·W<sub>ho</sub> + b<sub>o</sub> → softmax(y) = class probabilities.</li>
        </ol>
        <p class="small">
          Links are colored by sign (blue = +, red = −) and scaled by |weight|.
          Dimmed links come from pixels that are 0. Bias and ReLU are annotated near nodes.
        </p>
      </details>
    </div>
    <div>
      <canvas id="s7-net" width="560" height="340" aria-label="2x2 neural net diagram"></canvas>
      <figcaption class="small">
        Left: 2×2 inputs (squares). Mid: 3 hidden units (circles). Right: 4 output classes.
        Blue = positive weight, Red = negative, thicker = larger |w|.
      </figcaption>
      <div class="sketch" id="s7-bars" style="margin-top:.5rem"></div>
    </div>
  </div>
</section>


  <section id="s8" class="card">
    <div class="row">
      <div>
        <h3>8) Transformers & Attention — "Ask and Answer"</h3>
        <div class="status-box status-ok">Goals: tune which tokens are attended · read weights and the aggregated value vector V*.</div>
        <div class="status-box status-err">Pitfall: attention weights ≠ explanation of reasoning; it is a routing signal.</div>
        <p><strong>Concept.</strong> Each token selectively "looks at" other tokens by calculating attention weights.</p>
        <p><strong>Principle.</strong> Weighted sum of values: <span class="mono">softmax((Q·Kᵀ)/√d) · V</span>.</p>
        <div class="controls">
          <label>Focus token (Query) <select id="s8-focus" class="kbd"></select></label>
        </div>
        <details><summary>Key Principle (How it works)</summary>
          <p>We compute true dot‑product attention with tiny Q/K/V vectors. You pick the query token; we show attention weights to all keys and the resulting V* (weighted sum).</p>
        </details>
      </div>
      <div>
        <figure class="sketch" id="s8-sketch"></figure>
        <figcaption class="small">Bar chart = attention weights; text shows V*.</figcaption>
      </div>
    </div>
  </section>
  
  <h2 id="m3">Module 3: The LLM Pipeline</h2>

  <section id="s9" class="card">
    <h2>9) Training → Post‑training Pipeline</h2>
    <div class="status-box status-ok">Goals: separate pretraining (imitate) and post‑training (align).</div>
    <div class="status-box status-err">Pitfall: post‑training doesn’t re‑learn the world from scratch.</div>
    <p><strong>Concept.</strong> Pretraining learns to imitate text at scale; post‑training (SFT, preferences) teaches helpfulness and safety.</p>
    <p><strong>Principle.</strong> Same base model; behavior shaped by additional data and objectives.</p>
    <div class="controls"><button class="btn" id="s9-toggle">Toggle: Pretrain vs Post‑train</button></div>
    <figure class="sketch" id="s9-sketch"></figure>
  </section>
  
  <section id="s11" class="card">
    <div class="row">
      <div>
        <h3>11) Prompting Habit — TIC Template</h3>
        <div class="status-box status-ok">Goals: separate Task/Instructions/Context; build a clear prompt.</div>
        <div class="status-box status-err">Pitfall: mixing goals, style, and background in one sentence.</div>
        <p><strong>Concept.</strong> Make the <em>Task</em>, <em>Instructions</em>, and <em>Context</em> explicit.</p>
        <p><strong>Principle.</strong> Reduce ambiguity; add few‑shot examples when possible.</p>
        <div class="grid">
          <label>Task<textarea id="s11-t" rows="3" class="kbd" placeholder="Summarize the text."></textarea></label>
          <label>Instructions<textarea id="s11-i" rows="3" class="kbd" placeholder="In 3 bullet points, for a non-expert."></textarea></label>
          <label>Context<textarea id="s11-c" rows="3" class="kbd" placeholder="The text is about..."></textarea></label>
          <div><button class="btn" id="s11-build">Build prompt</button></div>
        </div>
      </div>
      <div>
        <figure class="sketch" id="s11-sketch"></figure>
        <figcaption class="small">A structured prompt card is generated here.</figcaption>
      </div>
    </div>
  </section>

  <section id="s12" class="card">
    <h2>12) Reasoning — Chain‑of‑Thought</h2>
    <div class="status-box status-ok">Goals: compare fast vs step‑by‑step on the same problem.</div>
    <div class="status-box status-err">Pitfall: CoT is not needed for trivial tasks.</div>
    <p><strong>Concept.</strong> Asking for steps often improves accuracy on hard problems.</p>
    <p><strong>Principle.</strong> More tokens allow intermediate computations; some models are trained to produce better steps.</p>
    <div class="controls"><button class="btn" id="s12-fast">Fast Answer</button> <button class="btn" id="s12-cot">Step‑by‑Step</button></div>
    <figure class="sketch" id="s12-sketch"></figure>
  </section>

  <h2 id="m4">Module 4: Advanced Application</h2>
  
  <section id="s15" class="card">
    <h2>15) What’s Behind One Prompt — Hidden Stack</h2>
    <div class="status-box status-ok">Goals: visualize layers that accompany your prompt.</div>
    <div class="status-box status-err">Pitfall: evaluating outputs without considering hidden context.</div>
    <p><strong>Concept.</strong> The response depends on <em>system rules</em>, <em>custom settings</em>, <em>thread history</em>, <em>retrieved docs</em>, and <em>tool results</em>, not just your last message.</p>
    <p><strong>Principle.</strong> Stable setups beat one‑off prompts.</p>
    <div class="controls">
      <label><input type="checkbox" class="s15" data-key="System Prompt"> System Prompt</label>
      <label><input type="checkbox" class="s15" data-key="Chat History" checked> Chat History</label>
      <label><input type="checkbox" class="s15" data-key="RAG Docs"> RAG Docs</label>
      <label><input type="checkbox" class="s15" data-key="Tool Results"> Tool Results</label>
    </div>
    <figure class="sketch" id="s15-sketch"></figure>
  </section>

  <section id="s16" class="card">
    <h2>16) Agents — Goal‑Directed Loops</h2>
    <div class="status-box status-ok">Goals: step through Plan→Act→Read→Adjust.</div>
    <div class="status-box status-err">Pitfall: autonomy without guardrails.</div>
    <p><strong>Concept.</strong> Plan → Act (tools) → Read → Adjust → (repeat).</p>
    <p><strong>Principle.</strong> Autonomy increases capability and risk; add guardrails.</p>
    <div class="controls"><button class="btn" id="s16-step">Step loop</button></div>
    <figure class="sketch" id="s16-sketch"></figure>
  </section>

  <section id="s17" class="card">
    <div class="row">
      <div>
        <h3>17) Creativity Knob — Temperature</h3>
        <div class="status-box status-ok">Goals: see how T reshapes the distribution; run two sampling experiments.</div>
        <div class="status-box status-err">Pitfall: confusing temperature with “randomness only”.</div>
        <p><strong>Concept.</strong> Low temperature → steadier answers; high → more variety.</p>
        <p><strong>Principle.</strong> Softmax with temperature rescales logits; higher T flattens the distribution.</p>
        <div class="controls">
          <label>T = <input id="s17-T" type="range" min="0.1" max="2.0" value="0.8" step="0.1"/> <span id="s17-Tv" class="pill">0.8</span></label>
          <button class="btn" id="s17-sample">Sample next word</button>
        </div>
        <details><summary>Experiment instructions</summary>
          <ol>
            <li>Set T=0.2 and click <em>Sample</em> five times. Record results.</li>
            <li>Set T=1.8 and click <em>Sample</em> five times. Compare diversity.</li>
          </ol>
        </details>
      </div>
      <div>
        <figure class="sketch" id="s17-sketch"></figure>
        <figcaption class="small">Absolute probabilities; sampling highlights the chosen row briefly.</figcaption>
      </div>
    </div>
  </section>
  
  </main>
  <script defer src="app.js"></script>
</body>
</html>